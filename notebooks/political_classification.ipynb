{"cells":[{"cell_type":"markdown","metadata":{"id":"BQ7_zrkOZuXv"},"source":["# **Unit 2 Assignment**: Feature Engineering \\& Supervised Classification\n","## *DATA 5420/6420*\n","\n","In this second assignment you will be tasked with training your own supervised classification model, this could be to do document classification of some sort, or a sentiment analysis. You will first be tasked with selecting a labeled text dataset to train a supervised classifier, then you will apply it to your dataset from Unit 1.\n","\n","Next, you will find a pretrained supervised model from Hugging Face, which has a larger collection of pretrained document classification and sentiment analysis models. You will investigate the results of the model you trained against the pretrained model and compare their performances. This will help you decide how you might incorporate some form of either document classification or sentiment analysis into your final product.\n","\n","**General breakdown of steps**:\n","\n","\n","1.   Select a labeled dataset to perform document classification or sentiment analysis\n","2.   Train at least two different models on the dataset, compare performance - If in the 6420 section, select at least 2 different models AND perform at least two steps of parameter tuning\n","3.   Apply the classification model to your dataset from Unit 1\n","4.   Examine results, speak to how well it appears to perform\n","5.   Apply a pretrained transformer model to your dataset from Unit 1\n","6.   Examine results, speak to how well it appears to perform\n","7.   Compare and contrast your trained model vs the pretrained model\n","\n","**Some suggested datasets for document classification**:\n","\n","\n","*   Brown Corpus -- accesible through NLTK\n","*   20 News Groups -- accessible through scikit learn\n","*   [Yelp Reviews Dataset](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset)\n","\n","**Some suggetsed datasets for sentiment analysis**:\n","\n","*   [IMDB movie reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n","*   [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)\n","*   Yelp Reviews Dataset - linked above\n","\n","You are by no means limited to these datasets, [Kaggle](https://www.kaggle.com/datasets) has lots of datasets available for document classification and sentiment analysis, so you may find something more relevant to your dataset there. Just make sure it it labeled data (i.e., has a labeled class like positive, negative).\n","\n","\n","**Pretrained Models**:\n","\n","You can find pretrained models for sentiment analysis and document classification on the models page for [HuggingFace](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending). Remember, tools like Poe, ChatGPT, Claude, etc. are excellent resources for developing code for implementing models such as these!!\n","\n","Try something like: *I need a pretrained model from hugging face to do XYZ, can you provide python code*"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"nn6gax7jZq-5"},"outputs":[],"source":["# import dependencies\n","# Import necessary libraries\n","import nltk\n","import re\n","from nltk.corpus import brown\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","# ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qB3Q5bJWvpsn"},"outputs":[],"source":["# load in your selected labeled dataset\n","df =  pd.read_csv('..\\data\\debates.csv')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_speech</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>good evening washington dc welcome unique even...</td>\n","      <td>democrat</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>come together tonight extraordinary time count...</td>\n","      <td>democrat</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>setting debate also different reduce unnecessa...</td>\n","      <td>democrat</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>come course four state florida arizona ohio il...</td>\n","      <td>democrat</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>well first heart go already lost someone suffe...</td>\n","      <td>democrat</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9117</th>\n","      <td>here believe believe verge greatest time alive...</td>\n","      <td>republican</td>\n","    </tr>\n","    <tr>\n","      <th>9118</th>\n","      <td>mr trump closing statement sir</td>\n","      <td>republican</td>\n","    </tr>\n","    <tr>\n","      <th>9119</th>\n","      <td>country serious trouble dont win anymore dont ...</td>\n","      <td>republican</td>\n","    </tr>\n","    <tr>\n","      <th>9120</th>\n","      <td>gentleman thank</td>\n","      <td>republican</td>\n","    </tr>\n","    <tr>\n","      <th>9121</th>\n","      <td>relieved nervous dont look relieved look get o...</td>\n","      <td>republican</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9122 rows × 2 columns</p>\n","</div>"],"text/plain":["                                           clean_speech       label\n","0     good evening washington dc welcome unique even...    democrat\n","1     come together tonight extraordinary time count...    democrat\n","2     setting debate also different reduce unnecessa...    democrat\n","3     come course four state florida arizona ohio il...    democrat\n","4     well first heart go already lost someone suffe...    democrat\n","...                                                 ...         ...\n","9117  here believe believe verge greatest time alive...  republican\n","9118                     mr trump closing statement sir  republican\n","9119  country serious trouble dont win anymore dont ...  republican\n","9120                                    gentleman thank  republican\n","9121  relieved nervous dont look relieved look get o...  republican\n","\n","[9122 rows x 2 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"N7484K9YvqNo"},"source":["**Will you be performing document classification or sentiment analysis? What is your outcome variable (i.e., positive, negative, genre type, etc.)**"]},{"cell_type":"markdown","metadata":{"id":"dZsSfH5rv25q"},"source":["I will be performing document classification. My outcome variable is political offiliation. "]},{"cell_type":"markdown","metadata":{"id":"hTR4UCSdvlne"},"source":["**Which dataset did you decide to go with and why?**"]},{"cell_type":"markdown","metadata":{"id":"1u2PnZ9Xv_vj"},"source":["I decided to go with presidential campaign transcripts. I just the appropriate label, either republican or democrat, based on which debate it was from. I read an academic paper that said this data makes great training data for this type of document classification. I think I still need to do some more careful preprocessing of the training data set, but I think where it is at now will work for this assignment. "]},{"cell_type":"markdown","metadata":{"id":"j40NCFwpwFct"},"source":["**What, if any cleaning or text normalization steps did you apply to this dataset and why?**"]},{"cell_type":"markdown","metadata":{"id":"kBmKaU1UwKAm"},"source":["You'll need to look in the train_data.ipynb file for the code, but I just did the more basic things. Removing stopwords, lowercase all words, removing all special characters, and lemmatization. I think I still need to chop off the beginnings and ends of each document, and I will do so before the final project. I may also break out documents into smaller segments. "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Uwj0TUHbxfq_"},"outputs":[],"source":["# perform feature engineering on your cleaned corpus\n","# Split data into text and labels\n","texts = df['clean_speech'].tolist()\n","labels = df['label'].tolist()\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["(7297, 10971)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Create a TF-IDF vectorizer\n","# Replace np.nan values with empty strings in X_train and X_test lists\n","X_train = ['' if x is None or pd.isna(x) else x for x in X_train]\n","X_test = ['' if x is None or pd.isna(x) else x for x in X_test]\n","\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(min_df=1, max_df=0.80)\n","\n","# Join the elements within each list\n","X_train_str = [' '.join(x) for x in X_train]\n","X_test_str = [' '.join(x) for x in X_test]\n","X_train_vec = vectorizer.fit_transform(X_train) # will determine the number of features\n","X_test_vec = vectorizer.transform(X_test) # will use the same number of features as X_train_vec\n","\n","num_features = X_train_vec.shape\n","num_features"]},{"cell_type":"markdown","metadata":{"id":"pxED96uWxkRA"},"source":["**Which form of feature engineering did you choose (count or TFIDF) and did you go with unigrams, bigrams, etc.? Why?**"]},{"cell_type":"markdown","metadata":{"id":"07FuHQb6xqvY"},"source":["I chose TFIDF and unigrams because that seemed like an excellent place to start. "]},{"cell_type":"markdown","metadata":{"id":"Q_EzBUnywYti"},"source":["**Next, train your supervised classifier. Remember:**\n","\n","\n","\n","*   Create at least a training and a test set (fine if you don't have enough data to do a validation set)\n","*   Perform cross-validation\n","*   Train at least two different supervised classifiers on your training set\n","*   If in the 6420 section, also plan to try out at least two changes to the model parameters\n","* Apply your best performing model to the test set\n","* Provide model evaluation metrics\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"39suNeoPw_Ex"},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression Accuracy: 0.8581\n","Linear SVM Accuracy: 0.8614\n","Random Forest Accuracy: 0.8405\n"]}],"source":["# fill in with coding steps to follow above instructions\n","# Define and train models\n","models = {\n","        \"Logistic Regression\": MultinomialNB(),\n","        \"Linear SVM\": LinearSVC(),\n","        \"Random Forest\": RandomForestClassifier(random_state=42)\n","}\n","\n","for name, model in models.items():\n","    model.fit(X_train_vec, y_train)\n","    y_pred = model.predict(X_test_vec)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f\"{name} Accuracy: {accuracy:.4f}\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 30 candidates, totalling 150 fits\n","[CV 1/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n","[CV 1/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.540 total time=   0.0s\n","[CV 2/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n","[CV 2/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.540 total time=   0.0s\n","[CV 3/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n","[CV 3/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.539 total time=   0.0s\n","[CV 4/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n","[CV 4/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.539 total time=   0.0s\n","[CV 5/5; 1/30] START C=0.001, loss=hinge, max_iter=1000.........................\n","[CV 5/5; 1/30] END C=0.001, loss=hinge, max_iter=1000;, score=0.539 total time=   0.0s\n","[CV 1/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n","[CV 1/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.540 total time=   0.0s\n","[CV 2/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n","[CV 2/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.540 total time=   0.0s\n","[CV 3/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n","[CV 3/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.539 total time=   0.0s\n","[CV 4/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n","[CV 4/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.539 total time=   0.0s\n","[CV 5/5; 2/30] START C=0.001, loss=hinge, max_iter=5000.........................\n","[CV 5/5; 2/30] END C=0.001, loss=hinge, max_iter=5000;, score=0.539 total time=   0.0s\n","[CV 1/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n","[CV 1/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.540 total time=   0.0s\n","[CV 2/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n","[CV 2/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.540 total time=   0.0s\n","[CV 3/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n","[CV 3/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.539 total time=   0.0s\n","[CV 4/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n","[CV 4/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.539 total time=   0.0s\n","[CV 5/5; 3/30] START C=0.001, loss=hinge, max_iter=10000........................\n","[CV 5/5; 3/30] END C=0.001, loss=hinge, max_iter=10000;, score=0.539 total time=   0.0s\n","[CV 1/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n","[CV 1/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.571 total time=   0.0s\n","[CV 2/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n","[CV 2/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.558 total time=   0.0s\n","[CV 3/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n","[CV 3/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.563 total time=   0.0s\n","[CV 4/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n","[CV 4/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.561 total time=   0.0s\n","[CV 5/5; 4/30] START C=0.001, loss=squared_hinge, max_iter=1000.................\n","[CV 5/5; 4/30] END C=0.001, loss=squared_hinge, max_iter=1000;, score=0.564 total time=   0.0s\n","[CV 1/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n","[CV 1/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.571 total time=   0.0s\n","[CV 2/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n","[CV 2/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.558 total time=   0.0s\n","[CV 3/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n","[CV 3/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.563 total time=   0.0s\n","[CV 4/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n","[CV 4/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.561 total time=   0.0s\n","[CV 5/5; 5/30] START C=0.001, loss=squared_hinge, max_iter=5000.................\n","[CV 5/5; 5/30] END C=0.001, loss=squared_hinge, max_iter=5000;, score=0.564 total time=   0.0s\n","[CV 1/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n","[CV 1/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.571 total time=   0.0s\n","[CV 2/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n","[CV 2/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.558 total time=   0.0s\n","[CV 3/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n","[CV 3/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.563 total time=   0.0s\n","[CV 4/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n","[CV 4/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.561 total time=   0.0s\n","[CV 5/5; 6/30] START C=0.001, loss=squared_hinge, max_iter=10000................\n","[CV 5/5; 6/30] END C=0.001, loss=squared_hinge, max_iter=10000;, score=0.564 total time=   0.0s\n","[CV 1/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n","[CV 1/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.542 total time=   0.0s\n","[CV 2/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n","[CV 2/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.545 total time=   0.0s\n","[CV 3/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n","[CV 3/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.543 total time=   0.0s\n","[CV 4/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n","[CV 4/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.542 total time=   0.0s\n","[CV 5/5; 7/30] START C=0.01, loss=hinge, max_iter=1000..........................\n","[CV 5/5; 7/30] END C=0.01, loss=hinge, max_iter=1000;, score=0.545 total time=   0.0s\n","[CV 1/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n","[CV 1/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.542 total time=   0.0s\n","[CV 2/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n","[CV 2/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.545 total time=   0.0s\n","[CV 3/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n","[CV 3/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.543 total time=   0.0s\n","[CV 4/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n","[CV 4/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.542 total time=   0.0s\n","[CV 5/5; 8/30] START C=0.01, loss=hinge, max_iter=5000..........................\n","[CV 5/5; 8/30] END C=0.01, loss=hinge, max_iter=5000;, score=0.545 total time=   0.0s\n","[CV 1/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n","[CV 1/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.542 total time=   0.0s\n","[CV 2/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n","[CV 2/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.545 total time=   0.0s\n","[CV 3/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n","[CV 3/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.543 total time=   0.0s\n","[CV 4/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n","[CV 4/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.542 total time=   0.0s\n","[CV 5/5; 9/30] START C=0.01, loss=hinge, max_iter=10000.........................\n","[CV 5/5; 9/30] END C=0.01, loss=hinge, max_iter=10000;, score=0.545 total time=   0.0s\n","[CV 1/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n","[CV 1/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.822 total time=   0.0s\n","[CV 2/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n","[CV 2/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.814 total time=   0.0s\n","[CV 3/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n","[CV 3/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.807 total time=   0.0s\n","[CV 4/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n","[CV 4/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.814 total time=   0.0s\n","[CV 5/5; 10/30] START C=0.01, loss=squared_hinge, max_iter=1000.................\n","[CV 5/5; 10/30] END C=0.01, loss=squared_hinge, max_iter=1000;, score=0.835 total time=   0.0s\n","[CV 1/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n","[CV 1/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.822 total time=   0.0s\n","[CV 2/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n","[CV 2/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.814 total time=   0.0s\n","[CV 3/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n","[CV 3/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.807 total time=   0.0s\n","[CV 4/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n","[CV 4/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.814 total time=   0.0s\n","[CV 5/5; 11/30] START C=0.01, loss=squared_hinge, max_iter=5000.................\n","[CV 5/5; 11/30] END C=0.01, loss=squared_hinge, max_iter=5000;, score=0.835 total time=   0.0s\n","[CV 1/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n","[CV 1/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.822 total time=   0.0s\n","[CV 2/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n","[CV 2/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.814 total time=   0.0s\n","[CV 3/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n","[CV 3/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.807 total time=   0.0s\n","[CV 4/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n","[CV 4/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.814 total time=   0.0s\n","[CV 5/5; 12/30] START C=0.01, loss=squared_hinge, max_iter=10000................\n","[CV 5/5; 12/30] END C=0.01, loss=squared_hinge, max_iter=10000;, score=0.835 total time=   0.0s\n","[CV 1/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n","[CV 1/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.847 total time=   0.0s\n","[CV 2/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n","[CV 2/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.832 total time=   0.0s\n","[CV 3/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n","[CV 3/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.826 total time=   0.0s\n","[CV 4/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n","[CV 4/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.843 total time=   0.0s\n","[CV 5/5; 13/30] START C=0.1, loss=hinge, max_iter=1000..........................\n","[CV 5/5; 13/30] END C=0.1, loss=hinge, max_iter=1000;, score=0.851 total time=   0.0s\n","[CV 1/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n","[CV 1/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.847 total time=   0.0s\n","[CV 2/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n","[CV 2/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.832 total time=   0.0s\n","[CV 3/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n","[CV 3/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.826 total time=   0.0s\n","[CV 4/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n","[CV 4/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.843 total time=   0.0s\n","[CV 5/5; 14/30] START C=0.1, loss=hinge, max_iter=5000..........................\n","[CV 5/5; 14/30] END C=0.1, loss=hinge, max_iter=5000;, score=0.851 total time=   0.0s\n","[CV 1/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n","[CV 1/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.847 total time=   0.0s\n","[CV 2/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n","[CV 2/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.832 total time=   0.0s\n","[CV 3/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n","[CV 3/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.826 total time=   0.0s\n","[CV 4/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n","[CV 4/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.843 total time=   0.0s\n","[CV 5/5; 15/30] START C=0.1, loss=hinge, max_iter=10000.........................\n","[CV 5/5; 15/30] END C=0.1, loss=hinge, max_iter=10000;, score=0.851 total time=   0.0s\n","[CV 1/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n","[CV 1/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.868 total time=   0.0s\n","[CV 2/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n","[CV 2/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.851 total time=   0.0s\n","[CV 3/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n","[CV 3/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.846 total time=   0.0s\n","[CV 4/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n","[CV 4/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.859 total time=   0.0s\n","[CV 5/5; 16/30] START C=0.1, loss=squared_hinge, max_iter=1000..................\n","[CV 5/5; 16/30] END C=0.1, loss=squared_hinge, max_iter=1000;, score=0.864 total time=   0.0s\n","[CV 1/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n","[CV 1/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.868 total time=   0.0s\n","[CV 2/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n","[CV 2/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.851 total time=   0.0s\n","[CV 3/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n","[CV 3/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.846 total time=   0.0s\n","[CV 4/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n","[CV 4/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.859 total time=   0.0s\n","[CV 5/5; 17/30] START C=0.1, loss=squared_hinge, max_iter=5000..................\n","[CV 5/5; 17/30] END C=0.1, loss=squared_hinge, max_iter=5000;, score=0.864 total time=   0.0s\n","[CV 1/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n","[CV 1/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.868 total time=   0.0s\n","[CV 2/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n","[CV 2/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.851 total time=   0.0s\n","[CV 3/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n","[CV 3/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.846 total time=   0.0s\n","[CV 4/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n","[CV 4/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.859 total time=   0.0s\n","[CV 5/5; 18/30] START C=0.1, loss=squared_hinge, max_iter=10000.................\n","[CV 5/5; 18/30] END C=0.1, loss=squared_hinge, max_iter=10000;, score=0.864 total time=   0.0s\n","[CV 1/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n","[CV 1/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.864 total time=   0.0s\n","[CV 2/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n","[CV 2/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.855 total time=   0.0s\n","[CV 3/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n","[CV 3/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.847 total time=   0.0s\n","[CV 4/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n","[CV 4/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.853 total time=   0.0s\n","[CV 5/5; 19/30] START C=1, loss=hinge, max_iter=1000............................\n","[CV 5/5; 19/30] END C=1, loss=hinge, max_iter=1000;, score=0.866 total time=   0.0s\n","[CV 1/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n","[CV 1/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.864 total time=   0.0s\n","[CV 2/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n","[CV 2/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.855 total time=   0.0s\n","[CV 3/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n","[CV 3/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.847 total time=   0.0s\n","[CV 4/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n","[CV 4/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.853 total time=   0.0s\n","[CV 5/5; 20/30] START C=1, loss=hinge, max_iter=5000............................\n","[CV 5/5; 20/30] END C=1, loss=hinge, max_iter=5000;, score=0.866 total time=   0.0s\n","[CV 1/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n","[CV 1/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.864 total time=   0.0s\n","[CV 2/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n","[CV 2/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.855 total time=   0.0s\n","[CV 3/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n","[CV 3/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.847 total time=   0.0s\n","[CV 4/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n","[CV 4/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.853 total time=   0.0s\n","[CV 5/5; 21/30] START C=1, loss=hinge, max_iter=10000...........................\n","[CV 5/5; 21/30] END C=1, loss=hinge, max_iter=10000;, score=0.866 total time=   0.0s\n","[CV 1/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n","[CV 1/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.868 total time=   0.0s\n","[CV 2/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n","[CV 2/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.855 total time=   0.0s\n","[CV 3/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n","[CV 3/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.843 total time=   0.0s\n","[CV 4/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n","[CV 4/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.857 total time=   0.0s\n","[CV 5/5; 22/30] START C=1, loss=squared_hinge, max_iter=1000....................\n","[CV 5/5; 22/30] END C=1, loss=squared_hinge, max_iter=1000;, score=0.867 total time=   0.0s\n","[CV 1/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n","[CV 1/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.868 total time=   0.0s\n","[CV 2/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n","[CV 2/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.855 total time=   0.0s\n","[CV 3/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n","[CV 3/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.843 total time=   0.0s\n","[CV 4/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n","[CV 4/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.857 total time=   0.0s\n","[CV 5/5; 23/30] START C=1, loss=squared_hinge, max_iter=5000....................\n","[CV 5/5; 23/30] END C=1, loss=squared_hinge, max_iter=5000;, score=0.867 total time=   0.0s\n","[CV 1/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n","[CV 1/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.868 total time=   0.0s\n","[CV 2/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n","[CV 2/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.855 total time=   0.0s\n","[CV 3/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n","[CV 3/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.843 total time=   0.0s\n","[CV 4/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n","[CV 4/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.857 total time=   0.0s\n","[CV 5/5; 24/30] START C=1, loss=squared_hinge, max_iter=10000...................\n","[CV 5/5; 24/30] END C=1, loss=squared_hinge, max_iter=10000;, score=0.867 total time=   0.0s\n","[CV 1/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n","[CV 1/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.845 total time=   0.0s\n","[CV 2/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n","[CV 2/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.832 total time=   0.1s\n","[CV 3/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n","[CV 3/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.818 total time=   0.0s\n","[CV 4/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n","[CV 4/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.833 total time=   0.0s\n","[CV 5/5; 25/30] START C=10, loss=hinge, max_iter=1000...........................\n","[CV 5/5; 25/30] END C=10, loss=hinge, max_iter=1000;, score=0.843 total time=   0.0s\n","[CV 1/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n","[CV 1/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.845 total time=   0.2s\n","[CV 2/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n","[CV 2/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.832 total time=   0.3s\n","[CV 3/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n","[CV 3/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.818 total time=   0.2s\n","[CV 4/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n","[CV 4/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.833 total time=   0.1s\n","[CV 5/5; 26/30] START C=10, loss=hinge, max_iter=5000...........................\n","[CV 5/5; 26/30] END C=10, loss=hinge, max_iter=5000;, score=0.844 total time=   0.2s\n","[CV 1/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n","[CV 1/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.845 total time=   0.1s\n","[CV 2/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n","[CV 2/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.832 total time=   0.3s\n","[CV 3/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n","[CV 3/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.818 total time=   0.2s\n","[CV 4/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n","[CV 4/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.833 total time=   0.2s\n","[CV 5/5; 27/30] START C=10, loss=hinge, max_iter=10000..........................\n","[CV 5/5; 27/30] END C=10, loss=hinge, max_iter=10000;, score=0.844 total time=   0.2s\n","[CV 1/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n","[CV 1/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.845 total time=   0.0s\n","[CV 2/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n","[CV 2/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.833 total time=   0.0s\n","[CV 3/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n","[CV 3/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.831 total time=   0.0s\n","[CV 4/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n","[CV 4/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.834 total time=   0.0s\n","[CV 5/5; 28/30] START C=10, loss=squared_hinge, max_iter=1000...................\n","[CV 5/5; 28/30] END C=10, loss=squared_hinge, max_iter=1000;, score=0.839 total time=   0.0s\n","[CV 1/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n","[CV 1/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.845 total time=   0.0s\n","[CV 2/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n","[CV 2/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.833 total time=   0.0s\n","[CV 3/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n","[CV 3/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.831 total time=   0.0s\n","[CV 4/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n","[CV 4/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.834 total time=   0.0s\n","[CV 5/5; 29/30] START C=10, loss=squared_hinge, max_iter=5000...................\n","[CV 5/5; 29/30] END C=10, loss=squared_hinge, max_iter=5000;, score=0.839 total time=   0.0s\n","[CV 1/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n","[CV 1/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.845 total time=   0.0s\n","[CV 2/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n","[CV 2/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.833 total time=   0.0s\n","[CV 3/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n","[CV 3/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.831 total time=   0.0s\n","[CV 4/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n","[CV 4/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.834 total time=   0.0s\n","[CV 5/5; 30/30] START C=10, loss=squared_hinge, max_iter=10000..................\n","[CV 5/5; 30/30] END C=10, loss=squared_hinge, max_iter=10000;, score=0.839 total time=   0.0s\n","Best parameters for LinearSVC: {'C': 1, 'loss': 'squared_hinge', 'max_iter': 1000}\n"]}],"source":["param_grid_svc = {\n","    'C': [0.001, 0.01, 0.1, 1, 10],\n","    'loss': ['hinge', 'squared_hinge'],\n","    'max_iter': [1000, 5000, 10000]\n","}\n","\n","# Grid search for LinearSVC\n","grid_search_svc = GridSearchCV(LinearSVC(), param_grid_svc, verbose = 10)\n","grid_search_svc.fit(X_train_vec, y_train)\n","\n","# After fitting, you would typically print the best parameters as follows:\n","print(\"Best parameters for LinearSVC:\", grid_search_svc.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"P3nft34Lw_fn"},"source":["**Which model performed best and how do you know?**"]},{"cell_type":"markdown","metadata":{"id":"gmpmkGDbxIKI"},"source":["THe linear SVM is performing the best. With the following parameters: {'C': 1, 'loss': 'squared_hinge', 'max_iter': 1000}."]},{"cell_type":"markdown","metadata":{"id":"7fJZUP7ixJJc"},"source":["**Now, bring in your dataset from Unit 1 and apply your best performing model to add labels to this dataset (sentiment or document class). Remember:**\n","\n","*   Apply the same cleaning and text normalization steps to this dataset as you did the training data\n","*   Apply the same feature engineering type and parameters\n","*   Use the `.transform()` on your Unit 1 dataset with the vectorizer to ensure you match the number of features used to train your model\n","*  Store the predictions and your text observations in a dataframe\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MOMUKMv634-5"},"outputs":[],"source":["# complete the above instructions\n","# read in reddit_data\n","reddit_df = pd.read_csv('reddit_data.csv')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cleaned_text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>mitch mcconnell lose control senate democrat s...</td>\n","      <td>Democrat</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>trump threatens ‘ leave country loses biden</td>\n","      <td>Democrat</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>demand kushner resign 'staggering ' level 'dep...</td>\n","      <td>Democrat</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>million people sign petition calling kkk decla...</td>\n","      <td>Democrat</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>report : biden admin discovers trump zero plan...</td>\n","      <td>Democrat</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        cleaned_text     label\n","0  mitch mcconnell lose control senate democrat s...  Democrat\n","1        trump threatens ‘ leave country loses biden  Democrat\n","2  demand kushner resign 'staggering ' level 'dep...  Democrat\n","3  million people sign petition calling kkk decla...  Democrat\n","4  report : biden admin discovers trump zero plan...  Democrat"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["reddit_df.head()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"np.nan is an invalid document, expected byte or unicode string.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Patri\\GitHub\\text\\class_assignments\\Unit 2 Assignment_TEMPLATE.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Patri/GitHub/text/class_assignments/Unit%202%20Assignment_TEMPLATE.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m reddit_vec \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(reddit_df[\u001b[39m'\u001b[39m\u001b[39mcleaned_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist())\n","File \u001b[1;32mc:\\Users\\Patri\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2150\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2133\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \n\u001b[0;32m   2135\u001b[0m \u001b[39mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2146\u001b[0m \u001b[39m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2147\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2148\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m, msg\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2150\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m   2151\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mtransform(X, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","File \u001b[1;32mc:\\Users\\Patri\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1428\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1425\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1427\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1428\u001b[0m _, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1430\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Patri\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n","File \u001b[1;32mc:\\Users\\Patri\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n","File \u001b[1;32mc:\\Users\\Patri\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    235\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[0;32m    237\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[1;32m--> 238\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n","\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."]}],"source":["reddit_vec = vectorizer.transform(reddit_df['cleaned_text'].tolist())"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"data":{"text/plain":["<349x10263 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 3144 stored elements in Compressed Sparse Row format>"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["reddit_vec"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["'cleaned_text' is predicted as 'Democrat' category.\n","'label' is predicted as 'Democrat' category.\n"]}],"source":["predicted_labels = grid_search_svc.predict(reddit_vec)\n","\n","for text, label in zip(reddit_df, predicted_labels):\n","    print(f\"'{text}' is predicted as '{label}' category.\")"]},{"cell_type":"markdown","metadata":{"id":"fjT7zU_j3_MN"},"source":["**Now examine your results, look at some individual observations and investigate whether the model predictions are logical/appear accurate. Describe your findings below:**"]},{"cell_type":"markdown","metadata":{"id":"ToUv8-y14LNo"},"source":[]},{"cell_type":"markdown","metadata":{"id":"A6yZIFQm4xJC"},"source":["**Now select a pretrained model from Hugging Face (linked above) and make predictions onto your Unit 1 dataset. Compare how it appears to perform against how the model you trained appeared to perform.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3b0clwx49zi"},"outputs":[],"source":["# download/import hugging face model\n","# apply to your dataset\n","# store the predictions as another column in your corpus dataframe"]},{"cell_type":"markdown","metadata":{"id":"hM4KEiF949Fy"},"source":[]},{"cell_type":"markdown","metadata":{"id":"4zkcXV855Ixi"},"source":["**How could you incorporate supervised classification (document or sentiment classification) into a product? -- think about what it could be useful for as we continue to work towards your final project.**"]},{"cell_type":"markdown","metadata":{"id":"eycly46R5XLC"},"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOywVXX//8zVNqv6RRm1NLx","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
